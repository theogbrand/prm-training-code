#!/bin/bash
#PBS -q AISG_large
#PBS -j oe
#PBS -k oed
#PBS -N qwen-sft-32b-training
#PBS -o pbs_queue_logs/
#PBS -l select=1:ngpus=8:ncpus=112:mem=2000gb
#PBS -l walltime=1440:00:00

cd $PBS_O_WORKDIR

uid="$(date +%Y%m%d_%H%M%S)"
base_model="Qwen/Qwen2.5-VL-7B-Instruct"
dataset_name="ob11/visual-prm-training-data-v1-mc0.0-qwen-format"
lr=2e-5 # -> not used now, using default lr from HFTrainer
epochs=2 
micro_batch_size=1
gradient_accumulation_steps=4
max_steps=-1 # -> not used now
min_lr=0 # -> not used now
weight_decay=1e-4 # -> not used now
push_to_hub=false

accelerate launch --config_file=train/deepspeed_zero3.yaml \
    train/sft_qwen.py \
    --dataset_name ${dataset_name} \
    --model_name_or_path ${base_model} \
    --per_device_train_batch_size ${micro_batch_size} \
    --gradient_accumulation_steps ${gradient_accumulation_steps} \
    --output_dir training_outputs/sft-qwen-7b-instruct-${uid} \
    --bf16 True \
    --torch_dtype bfloat16 \
    --gradient_checkpointing \
    --num_train_epochs ${epochs} 